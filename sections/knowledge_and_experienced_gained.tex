\section{Knowledge And Experience Gained}
\subsection{Technical Knowledge Gained From Assignments}
\noindent
The first technical knowledge I gained was learning about NLP. I had the opportunity to learn different unsupervised clustering techniques, such as HDBSCAN, Agglomerative, and DBSCAN. I also had hands-on experience with Tensorflow and PyTorch to use BERT and ALBERT to convert text into embeddings. Another opportunity I had was to do Topic Modeling for positive and negative sentiments with one of the datasets using Gensim and Mallet. I later learned about using TextRank as an alternatives for Topic Modeling.

\noindent
The second technical knowledge I gained was developing the web-based platform from scratch. I also had to do mock-ups and conduct user testing to check whether my designs would be intuitive for end users. Having taken CS3216, I wanted to apply the technologies and practices I learned from my seniors when I worked on projects with them. I am current using ReactJS with Typescript, Flask, and MongoDB. The application is deployed over Docker. I also integrated a queue based system using RQ to managed processes that requires long computation time in the background.

\noindent
The third technical knowledge I gained was deploying the application over Docker and Kubernetes (AWS EKS) on AWS. I also setup my unit and regression tests to use Docker Compose so that the testing is also containerized. This allows continuous integration to done before deployments without having to worry about system requirements. Having the opportunity to deploy the application on Kubernetes, I learned that Kubernetes allows the application to scale easily, update and deploy new images easily, and monitor the health of the application easily through Kubernetes dashboard. I also learned about AWS Fargate to manage the containers and instances type even though I did not use it for deployment. 

\noindent
The fourth technical knowledge I gained is using PySpark to build scalable models for the pipeline. As the training dataset can have over 400,000 utterances and the RAM usage is exponential to the number of utterances, performing clustering on the 400,000 utterances can take up more than 200GB RAM. While I converted my original pipeline using Pandas, Scikit Learn and PyTorch to PySpark, I learned to about the capabilities of PySpark to distribute the memory across multiple platforms, making the pipeline highly scalable despite some memory and computation overheads. I currently using PySpark to perform text analytics and embeddings extraction, giving the same pipeline as before but scalable to large datasets. The PySpark code is deployed on AWS Glue which provides managed ETL services, solving the pain of not having enough RAM.

\subsection{Organizational/Industry Experience Gained From Assignments}
\noindent
The first industrial experience I gained was sharing my NLP results with government agencies. The very first presentation I gave was to MOE, but that was a last minute notice and I did not have any slides prepared. During the meeting, I could tell the audience could not follow. After that incident, I prepared a PowerPoint template to share my results that is easy to understand for end users. When I finally presented my results to MOE and IRAS again, there was a huge difference and the audience were asking questions that showed they understood my sharing. Most importantly, the government agencies know how to use the CSV files to clean their chatbot's dataset. 

\noindent 
The second industrial experience I gained was working with the team. My team has daily stand-ups which I was unused to at first, as I was clueless the first few weeks and did not know how to report my activities in NLP. After asking around and observing how others report, I was able to. My team also uses JIRA to track issues and also uses it for sprint planning. It took me about a week to adjust because I do not use such platform often in school. The most exciting part was emergency request to develop a chatbot within 2 days to disseminate information about masks due to the Coronavirus situation in Singapore. I first-hand saw how a single developer was able to build it in 2 days. Even though I training the chatbot through DialogFlow that 2 days, I helped out with redesigning the chatbot interface, scraping MOH web contents to populate the statistics into the database, and automating the regression testing for the training dataset.

\noindent
The third industrial experience I gained was understanding the terminologies that government agencies used. In my first team meeting, the Director was present to supervise the team's progress. During the team meeting, there were many acronyms thrown that left me confused. I was not familiar with many of the government agencies and little did I know the purpose of each of these agencies. However, I was able to follow the conversations in subsequent meetings as I became more familiar with the terminologies.


\subsection{Areas of Applicability of Knowledge And Experienced Gained}
\noindent
NLP is a niche field in Machine Learning and Deep Learning, but is growing rapidly. NLP has directly applications to any projects related to developing chatbot and understanding consumers' sentiments. The concepts and skills in Machine Learning and Deep Knowledge, however, is applicable to other fields, such as Computer Vision. As I hoped to develop a talking robot that can conversational capabilities of a human one day, I believe the NLP skills I gained at GovTech would help me in future.
